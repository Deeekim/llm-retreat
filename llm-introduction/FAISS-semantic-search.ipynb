{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "824719db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: x['is_pull_request'] == False and len(x['comments']) > 0\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90a717c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "11c8a91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'html_url': ['https://github.com/huggingface/datasets/issues/2945',\n",
       "  'https://github.com/huggingface/datasets/issues/2943'],\n",
       " 'title': ['Protect master branch',\n",
       "  'Backwards compatibility broken for cached datasets that use `.filter()`'],\n",
       " 'comments': [['Cool, I think we can do both :)',\n",
       "   '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).'],\n",
       "  [\"Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\\r\\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?\",\n",
       "   \"If it's easy enough to implement, then yes please ðŸ˜„  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests.\",\n",
       "   \"Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR\",\n",
       "   \"I just merged a fix, let me know if you're still having this kind of issues :)\\r\\n\\r\\nWe'll do a release soon to make this fix available\",\n",
       "   'Definitely works on several manual cases with our dummy datasets, thank you @lhoestq !',\n",
       "   'Fixed by #2947.']],\n",
       " 'body': ['After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\\r\\n- 00cc036fea7c7745cfe722360036ed306796a3f2\\r\\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\\r\\n- ...\\r\\n\\r\\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\\r\\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\\r\\n  - Currently, simple merge commits are already disabled\\r\\n  - I propose to disable rebase merging as well\\r\\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\\r\\n  - ~~This protection would reject direct pushes to master branch~~\\r\\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\\r\\n- [x] Protect the master branch only from direct pushing of **merge commits**\\r\\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\\r\\n  - No need to disable/re-enable this protection on each release \\r\\n\\r\\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.',\n",
       "  '## Describe the bug\\r\\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \\r\\n`ValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}`\\r\\n\\r\\nRelated feature: https://github.com/huggingface/datasets/pull/2836\\r\\n\\r\\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \\r\\n\\r\\n## Workaround\\r\\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\\r\\n\\r\\n## Steps to reproduce the bug\\r\\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\\r\\n\\r\\n2. `pip install datasets==1.11.0` and run the following snippet:\\r\\n\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\nids = [\"1272-141231-0000\"]\\r\\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\\r\\nds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n```\\r\\n3. `pip install datasets==1.12.1` and re-run the code again\\r\\n\\r\\n## Expected results\\r\\nSame result as with the previous `datasets` version.\\r\\n\\r\\n## Actual results\\r\\n```bash\\r\\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\\r\\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\\r\\nTraceback (most recent call last):\\r\\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\\r\\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\\r\\n    indices = self.map(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\\r\\n    return self._map_single(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\\r\\n    out = func(self, *args, **kwargs)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\\r\\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\\r\\n    return cls(\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\\r\\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\\r\\n    return Features(recursive_reorder(self, other))\\r\\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\\r\\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\\r\\nValueError: Keys mismatch: between {\\'indices\\': Value(dtype=\\'uint64\\', id=None)} and {\\'file\\': Value(dtype=\\'string\\', id=None), \\'text\\': Value(dtype=\\'string\\', id=None), \\'speaker_id\\': Value(dtype=\\'int64\\', id=None), \\'chapter_id\\': Value(dtype=\\'int64\\', id=None), \\'id\\': Value(dtype=\\'string\\', id=None)}\\r\\n\\r\\nProcess finished with exit code 1\\r\\n\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n- `datasets` version: 1.12.1\\r\\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\\r\\n- Python version: 3.8.10\\r\\n- PyArrow version: 5.0.0\\r\\n']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ada10b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2401a9d09d946458815576f138715bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/808 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def explode(batch):\n",
    "    exploded = []\n",
    "    for i in range(len(batch['comments'])):\n",
    "        for idx in range(len(batch['comments'][i])):\n",
    "            new_item = {}\n",
    "            for key in batch:\n",
    "                if key == 'comments':\n",
    "                    new_item[key] = batch[key][i][idx]\n",
    "                else:\n",
    "                    new_item[key] = batch[key][i]\n",
    "            exploded.append(new_item)\n",
    "    return {k: [dic[k] for dic in exploded] for k in exploded[0]}\n",
    "\n",
    "comments_dataset = issues_dataset.map(explode, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c4e7c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9558f465bd749b8a4efe1cd2ae61c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89cda2626fc46b4b2b8977c47880b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out really short comments that most often are not helpful\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {'comment_length': len(x['comments'].split())}\n",
    ")\n",
    "\n",
    "comments_dataset = comments_dataset.filter(\n",
    "        lambda x: x['comment_length'] > 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fd7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17486d78a2034b49835269a4fe04ec95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4ff5d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748e455b2b2a46428690c415c341352d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed0d11d496d43f3a84c7bdaf4794b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0738ee05b254f1686e6784bdcfa7fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc6c2c26eeb4942b7e58d102dd0fc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6242097f1d934625a41b7e3de8fb4871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c1d060f635443aaaa260a11550564a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_states[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding = True, truncation = True, return_tensors = \"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k,v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x['text']).detach().cpu().numpy()[0]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset.add_faiss_index(column = \"embeddings\")\n",
    "\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending = False, inplace = True)\n",
    "\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
